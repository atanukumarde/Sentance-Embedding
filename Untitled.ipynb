{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence :  ['I ate dinner.', 'We had a three-course meal.', 'Brad came to dinner with us.', 'He loves fish tacos.', 'In the end, we all felt like we ate too much.', 'We all agreed; it was a magnificent evening.']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I ate dinner.\", \n",
    "       \"We had a three-course meal.\", \n",
    "       \"Brad came to dinner with us.\",\n",
    "       \"He loves fish tacos.\",\n",
    "       \"In the end, we all felt like we ate too much.\",\n",
    "       \"We all agreed; it was a magnificent evening.\"]\n",
    "print(\"Sentence : \",sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of each document\n",
    "tokenized_sent = []\n",
    "for s in sentences:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))\n",
    "#tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Gensim (Optional)\n",
    "#!python -m pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\49177\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied: requests in c:\\users\\49177\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'ate', 'dinner', '.'], tags=[0]),\n",
       " TaggedDocument(words=['we', 'had', 'a', 'three-course', 'meal', '.'], tags=[1]),\n",
       " TaggedDocument(words=['brad', 'came', 'to', 'dinner', 'with', 'us', '.'], tags=[2]),\n",
       " TaggedDocument(words=['he', 'loves', 'fish', 'tacos', '.'], tags=[3]),\n",
       " TaggedDocument(words=['in', 'the', 'end', ',', 'we', 'all', 'felt', 'like', 'we', 'ate', 'too', 'much', '.'], tags=[4]),\n",
       " TaggedDocument(words=['we', 'all', 'agreed', ';', 'it', 'was', 'a', 'magnificent', 'evening', '.'], tags=[5])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': <gensim.models.keyedvectors.Vocab at 0x14aec1f71c0>,\n",
       " 'ate': <gensim.models.keyedvectors.Vocab at 0x14aebbcbb20>,\n",
       " 'dinner': <gensim.models.keyedvectors.Vocab at 0x14aebbcbcd0>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x14aebbcbf70>,\n",
       " 'we': <gensim.models.keyedvectors.Vocab at 0x14aebbcbfa0>,\n",
       " 'had': <gensim.models.keyedvectors.Vocab at 0x14aebbcbc40>,\n",
       " 'a': <gensim.models.keyedvectors.Vocab at 0x14aed26b070>,\n",
       " 'three-course': <gensim.models.keyedvectors.Vocab at 0x14aed26b0d0>,\n",
       " 'meal': <gensim.models.keyedvectors.Vocab at 0x14aed26b130>,\n",
       " 'brad': <gensim.models.keyedvectors.Vocab at 0x14aed26b190>,\n",
       " 'came': <gensim.models.keyedvectors.Vocab at 0x14aed26b1f0>,\n",
       " 'to': <gensim.models.keyedvectors.Vocab at 0x14aed26b250>,\n",
       " 'with': <gensim.models.keyedvectors.Vocab at 0x14aed26b2b0>,\n",
       " 'us': <gensim.models.keyedvectors.Vocab at 0x14aed26b310>,\n",
       " 'he': <gensim.models.keyedvectors.Vocab at 0x14aed26b370>,\n",
       " 'loves': <gensim.models.keyedvectors.Vocab at 0x14aed26b3d0>,\n",
       " 'fish': <gensim.models.keyedvectors.Vocab at 0x14aed26b430>,\n",
       " 'tacos': <gensim.models.keyedvectors.Vocab at 0x14aed26b490>,\n",
       " 'in': <gensim.models.keyedvectors.Vocab at 0x14aed26b4f0>,\n",
       " 'the': <gensim.models.keyedvectors.Vocab at 0x14aed26b550>,\n",
       " 'end': <gensim.models.keyedvectors.Vocab at 0x14aed26b5b0>,\n",
       " ',': <gensim.models.keyedvectors.Vocab at 0x14aed26b610>,\n",
       " 'all': <gensim.models.keyedvectors.Vocab at 0x14aed26b670>,\n",
       " 'felt': <gensim.models.keyedvectors.Vocab at 0x14aed26b6d0>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x14aed26b730>,\n",
       " 'too': <gensim.models.keyedvectors.Vocab at 0x14aed26b790>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x14aed26b7f0>,\n",
       " 'agreed': <gensim.models.keyedvectors.Vocab at 0x14aed26b850>,\n",
       " ';': <gensim.models.keyedvectors.Vocab at 0x14aed26b8b0>,\n",
       " 'it': <gensim.models.keyedvectors.Vocab at 0x14aed26b910>,\n",
       " 'was': <gensim.models.keyedvectors.Vocab at 0x14aed26b970>,\n",
       " 'magnificent': <gensim.models.keyedvectors.Vocab at 0x14aed26b9d0>,\n",
       " 'evening': <gensim.models.keyedvectors.Vocab at 0x14aed26ba30>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''\n",
    "\n",
    "## Print model vocabulary\n",
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similirity: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 0.6410925388336182),\n",
       " (0, 0.33706802129745483),\n",
       " (4, 0.254173219203949),\n",
       " (2, 0.20233546197414398),\n",
       " (5, 0.09914921224117279),\n",
       " (1, 0.051740117371082306)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Similirity: \")\n",
    "test_doc = word_tokenize(\"I had pizza and pasta\".lower())\n",
    "test_doc_vector = model.infer_vector(test_doc)\n",
    "model.docvecs.most_similar(positive = [test_doc_vector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch torchvision cudatoolkit=10.2 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\49177\\anaconda3\\lib\\site-packages (0.3.7.2)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\49177\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\49177\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.18.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\49177\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.47.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\49177\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\49177\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.23.1)\n",
      "Requirement already satisfied: transformers<3.4.0,>=3.1.0 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.3.1)\n",
      "Requirement already satisfied: future in c:\\users\\49177\\anaconda3\\lib\\site-packages (from torch>=1.2.0->sentence-transformers) (0.18.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\49177\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\49177\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (2020.6.8)\n",
      "Requirement already satisfied: click in c:\\users\\49177\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.8.1rc2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\49177\\anaconda3\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in c:\\users\\49177\\anaconda3\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (20.4)\n",
      "Requirement already satisfied: requests in c:\\users\\49177\\anaconda3\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\49177\\anaconda3\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: six in c:\\users\\49177\\anaconda3\\lib\\site-packages (from sacremoses->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = sbert_model.encode(sentences)\n",
    "\n",
    "#print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
    "#print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I had pizza and pasta\"\n",
    "query_vec = sbert_model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  I ate dinner. ; similarity =  0.71734613\n",
      "Sentence =  We had a three-course meal. ; similarity =  0.637134\n",
      "Sentence =  Brad came to dinner with us. ; similarity =  0.5897909\n",
      "Sentence =  He loves fish tacos. ; similarity =  0.62239355\n",
      "Sentence =  In the end, we all felt like we ate too much. ; similarity =  0.41980496\n",
      "Sentence =  We all agreed; it was a magnificent evening. ; similarity =  0.18081596\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "  sim = cosine(query_vec, sbert_model.encode([sent])[0])\n",
    "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-hub in c:\\users\\49177\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from tensorflow-hub) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from tensorflow-hub) (1.18.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\49177\\anaconda3\\lib\\site-packages (from tensorflow-hub) (3.13.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\49177\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow-hub) (49.2.0.post20200714)\n"
     ]
    }
   ],
   "source": [
    "# Install TF-Hub.\n",
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 170.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 330.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 490.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 650.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 810.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 960.00MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "#\"https://tfhub.dev/google/universal-sentence-encoder/1\"\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model(sentences)\n",
    "query = \"I had pizza and pasta\"\n",
    "query_vec = model([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  I ate dinner. ; similarity =  0.4686642\n",
      "Sentence =  We had a three-course meal. ; similarity =  0.35643065\n",
      "Sentence =  Brad came to dinner with us. ; similarity =  0.20338944\n",
      "Sentence =  He loves fish tacos. ; similarity =  0.16515438\n",
      "Sentence =  In the end, we all felt like we ate too much. ; similarity =  0.14987424\n",
      "Sentence =  We all agreed; it was a magnificent evening. ; similarity =  0.05843591\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "  sim = cosine(query_vec, model([sent])[0])\n",
    "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
