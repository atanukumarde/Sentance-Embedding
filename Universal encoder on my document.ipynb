{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = codecs.open('C:/Users/49177/Desktop/advance project/sample.txt', 'r', 'utf-8')\n",
    "content = doc.read()\n",
    "#content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MRD005-340001- Advanced Project 1\\r\\nReport\\r\\nSentence Embedding for text mining\\r\\nAtanu Kumar De – a.de@jacobs-university.de\\r\\nExecutive Summary\\r\\nThis report is a summary of our work on the Advanced Project 1, which is a part of our coursework in the second semester of Data Engineering.\\r\\nFor the project initially, we wanted to detect sentence embedding techniques. The idea is to tag a sentence from with some values to able to represent them in multi-dimensional vector space to infer on the similarity between two sentences.\\r\\nThree techniques have been detected so far, using Doc2Vec, Sentence-BERT and Google Universal Sentence Encoder.\\u2003\\r\\n\\r\\n1. Introduction\\r\\nThe purpose of this project is to perform sentence embedding. The subsequent sections of this report explain in detail the project. 3 different sentence embedding technique has been implemented on a sample sentence. The results of the trained model have been  illustrated. The output and codes will be uploaded in GitHub. \\r\\n2. Word embedding to Sentence embedding\\r\\n2.1 Word Embedding\\r\\nTo perform natural language processing and text mining words are represented as vectors which can be represented in multi-dimensional space. This has already been implemented in some pre trained model like Word2Vec, Glove etc.[1] \\r\\n\\r\\n2.2 Drawbacks of word embedding\\r\\n\\r\\nLot of times it happens that only tagging the word to a vector and inferring is not enough as words have their own contextual meaning and the position of a word in a language plays an important role. So to make the system understand the semantic of a sentence. So many models live Word2Vec has been extended to models like Doc2Vec.[2]\\r\\n\\r\\n2.3 Sentence Embedding\\r\\n\\r\\nThe new way of representing text become sentence encoding in which a long sentence is not directly hot encoded with one single vector, rather the weighted average of the of individual words is being considered while embedding the sentence with dimensionality reduction.[3]\\r\\n \\r\\n\\r\\n3. Sentence Embedding Techniques\\r\\n\\r\\n3.1 Doc2Vec[4]\\r\\n\\r\\nThe working principle of Doc2Vec is based on the representation of paragraph vector. This kind of unsupervised model learns from inferring from subparts of a complete text. The length of the paragraph can vary between sentence, paragraph, or complete document. This sentence embedding technique can be applied in two ways.\\r\\n\\r\\n3.1.1 Distributed memory model\\r\\n\\r\\nIn this type of implementation, the general idea is to predict the most relevant sentence considering the average of word vectors. So, while predicting next word or missing word in a sentence it remembers the information collected during the whole process and helps in further inference finding between two different sentences.\\r\\n\\r\\n \\r\\n\\r\\n3.1.2 Distributed bag of words\\r\\n\\r\\nThe learning technique of this model is completely opposite to the above mention one. It takes up a random sample word and tries to predict its nearby windows with the help of similar occurring words in the document. \\r\\n\\r\\n \\r\\n   Figure 2: Steps in PVDOBW\\r\\n      Source: https://cs.stanford.edu/~quocle/paragraph_vector.pdf\\r\\n\\r\\n\\r\\n3.2 Sentence-BERT[5]\\r\\n\\r\\nAn extended version of the BERT model is SBERT. In BERT models an input sentence needed to compare with huge number of sentences that are already present which is been avoided and the time complexity   is reduced to milliseconds from hours in Sentence-BERT.\\r\\nThis technique can be used for both regression as well as classification tasks. While modeling for regression tasks cosine similarity of the embedding vector is considered and SoftMax classification technique for classification tasks.\\r\\n\\r\\n\\r\\n \\r\\n          \\r\\n\\r\\n3.3 Universal Sentence Encoder\\r\\n\\r\\nWith developments around sentence embedding Google came up with this library mainly for the purpose of transfer learning to speed up training time by reducing the required supervised training data. There are two type of model architecture based one transformer-based sentence encoding and Deep Averaging Network based encoding.[6]\\r\\n\\r\\n3.3.1 Transformer-based sentence encoding model\\r\\n\\r\\nFor both of model sentences are being cleaned and tokenized before feeding them to the model. In case of transformer-based model, which is built on recurrent or convolution neural network. So, for every layer the model keeps on adding different information also known attention addition for layers. [7]\\r\\n\\r\\n \\r\\n\\r\\n3.3.2 Deep Averaging Network\\r\\n\\r\\nIn case of deep average network, the embeddings consider the vector average associated with the input sentences. Eventually after passing that multi-dimensional vector representation through the sequential layers of the model, the resulting classification is done in the last layer of the neural network.[8]\\r\\n\\r\\n4. Model implementation and Output\\r\\n\\r\\n4.1 Sample sentences list as training set\\r\\n4.1 Sample sentences as test\\r\\n“I had pizza and pasta”\\r\\n4.2 Model Doc2Vec\\r\\n4.3 Model Sentence-BERT\\r\\n4.3 Model Universal Sentence Encoder\\r\\n9. Future Scope\\r\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r'[\\r\\t]')\n",
    "content = regex.sub(\" \", content)\n",
    "\n",
    "content = re.sub(r'\\d+', '', content)           #remove the digits in text\n",
    "\n",
    "content = re.sub(r'\\n', '. ', content)          #change the newline in text to fullstop \n",
    "\n",
    "txtfile2=[]\n",
    "\n",
    "\n",
    "for wd in content.split('. '):\n",
    "    txtfile2.append(wd.strip())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove blank sentances from the list\n",
    "while '' in txtfile2:\n",
    "    txtfile2.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MRD-- Advanced Project',\n",
       " 'Report',\n",
       " 'Sentence Embedding for text mining',\n",
       " 'Atanu Kumar De – a.de@jacobs-university.de',\n",
       " 'Executive Summary',\n",
       " 'This report is a summary of our work on the Advanced Project , which is a part of our coursework in the second semester of Data Engineering',\n",
       " 'For the project initially, we wanted to detect sentence embedding techniques',\n",
       " 'The idea is to tag a sentence from with some values to able to represent them in multi-dimensional vector space to infer on the similarity between two sentences',\n",
       " 'Three techniques have been detected so far, using DocVec, Sentence-BERT and Google Universal Sentence Encoder.',\n",
       " 'Introduction',\n",
       " 'The purpose of this project is to perform sentence embedding',\n",
       " 'The subsequent sections of this report explain in detail the project',\n",
       " 'different sentence embedding technique has been implemented on a sample sentence',\n",
       " 'The results of the trained model have been  illustrated',\n",
       " 'The output and codes will be uploaded in GitHub',\n",
       " 'Word embedding to Sentence embedding',\n",
       " 'Word Embedding',\n",
       " 'To perform natural language processing and text mining words are represented as vectors which can be represented in multi-dimensional space',\n",
       " 'This has already been implemented in some pre trained model like WordVec, Glove etc.[]',\n",
       " 'Drawbacks of word embedding',\n",
       " 'Lot of times it happens that only tagging the word to a vector and inferring is not enough as words have their own contextual meaning and the position of a word in a language plays an important role',\n",
       " 'So to make the system understand the semantic of a sentence',\n",
       " 'So many models live WordVec has been extended to models like DocVec.[]',\n",
       " 'Sentence Embedding',\n",
       " 'The new way of representing text become sentence encoding in which a long sentence is not directly hot encoded with one single vector, rather the weighted average of the of individual words is being considered while embedding the sentence with dimensionality reduction.[]',\n",
       " 'Sentence Embedding Techniques',\n",
       " 'DocVec[]',\n",
       " 'The working principle of DocVec is based on the representation of paragraph vector',\n",
       " 'This kind of unsupervised model learns from inferring from subparts of a complete text',\n",
       " 'The length of the paragraph can vary between sentence, paragraph, or complete document',\n",
       " 'This sentence embedding technique can be applied in two ways',\n",
       " '.',\n",
       " 'Distributed memory model',\n",
       " 'In this type of implementation, the general idea is to predict the most relevant sentence considering the average of word vectors',\n",
       " 'So, while predicting next word or missing word in a sentence it remembers the information collected during the whole process and helps in further inference finding between two different sentences',\n",
       " '.',\n",
       " 'Distributed bag of words',\n",
       " 'The learning technique of this model is completely opposite to the above mention one',\n",
       " 'It takes up a random sample word and tries to predict its nearby windows with the help of similar occurring words in the document',\n",
       " 'Figure : Steps in PVDOBW',\n",
       " 'Source: https://cs.stanford.edu/~quocle/paragraph_vector.pdf',\n",
       " 'Sentence-BERT[]',\n",
       " 'An extended version of the BERT model is SBERT',\n",
       " 'In BERT models an input sentence needed to compare with huge number of sentences that are already present which is been avoided and the time complexity   is reduced to milliseconds from hours in Sentence-BERT',\n",
       " 'This technique can be used for both regression as well as classification tasks',\n",
       " 'While modeling for regression tasks cosine similarity of the embedding vector is considered and SoftMax classification technique for classification tasks',\n",
       " 'Universal Sentence Encoder',\n",
       " 'With developments around sentence embedding Google came up with this library mainly for the purpose of transfer learning to speed up training time by reducing the required supervised training data',\n",
       " 'There are two type of model architecture based one transformer-based sentence encoding and Deep Averaging Network based encoding.[]',\n",
       " '.',\n",
       " 'Transformer-based sentence encoding model',\n",
       " 'For both of model sentences are being cleaned and tokenized before feeding them to the model',\n",
       " 'In case of transformer-based model, which is built on recurrent or convolution neural network',\n",
       " 'So, for every layer the model keeps on adding different information also known attention addition for layers',\n",
       " '[]',\n",
       " '.',\n",
       " 'Deep Averaging Network',\n",
       " 'In case of deep average network, the embeddings consider the vector average associated with the input sentences',\n",
       " 'Eventually after passing that multi-dimensional vector representation through the sequential layers of the model, the resulting classification is done in the last layer of the neural network.[]',\n",
       " 'Model implementation and Output',\n",
       " 'Sample sentences list as training set',\n",
       " 'Sample sentences as test',\n",
       " '“I had pizza and pasta”',\n",
       " 'Model DocVec',\n",
       " 'Model Sentence-BERT',\n",
       " 'Model Universal Sentence Encoder',\n",
       " 'Future Scope']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfile2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(67, 512), dtype=float32, numpy=\n",
       "array([[ 0.05907499, -0.01788058, -0.0005632 , ..., -0.05677126,\n",
       "         0.00624436, -0.05240188],\n",
       "       [-0.07234326,  0.04950103,  0.02009975, ..., -0.06924707,\n",
       "        -0.08039244, -0.01377062],\n",
       "       [ 0.02081599, -0.03649791, -0.00404722, ...,  0.01070163,\n",
       "        -0.05335678, -0.01547295],\n",
       "       ...,\n",
       "       [-0.02353393,  0.00792683,  0.0200094 , ..., -0.00778065,\n",
       "         0.05566706, -0.03009693],\n",
       "       [-0.00475653, -0.02128287, -0.06763035, ..., -0.01462175,\n",
       "         0.07259814, -0.05311895],\n",
       "       [ 0.03975759, -0.05436413, -0.07564493, ..., -0.04395555,\n",
       "         0.02832518,  0.00050319]], dtype=float32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings = model(txtfile2)\n",
    "query = \"why sentence encoding\"\n",
    "query_vec = model([query])[0]\n",
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  MRD-- Advanced Project ; similarity =  0.04695433\n",
      "Sentence =  Report ; similarity =  0.040158734\n",
      "Sentence =  Sentence Embedding for text mining ; similarity =  0.36185965\n",
      "Sentence =  Atanu Kumar De – a.de@jacobs-university.de ; similarity =  0.08948345\n",
      "Sentence =  Executive Summary ; similarity =  0.16574596\n",
      "Sentence =  This report is a summary of our work on the Advanced Project , which is a part of our coursework in the second semester of Data Engineering ; similarity =  0.060110137\n",
      "Sentence =  For the project initially, we wanted to detect sentence embedding techniques ; similarity =  0.21075705\n",
      "Sentence =  The idea is to tag a sentence from with some values to able to represent them in multi-dimensional vector space to infer on the similarity between two sentences ; similarity =  0.21623132\n",
      "Sentence =  Three techniques have been detected so far, using DocVec, Sentence-BERT and Google Universal Sentence Encoder. ; similarity =  0.28204095\n",
      "Sentence =  Introduction ; similarity =  0.059603408\n",
      "Sentence =  The purpose of this project is to perform sentence embedding ; similarity =  0.333629\n",
      "Sentence =  The subsequent sections of this report explain in detail the project ; similarity =  -0.036824994\n",
      "Sentence =  different sentence embedding technique has been implemented on a sample sentence ; similarity =  0.31401125\n",
      "Sentence =  The results of the trained model have been  illustrated ; similarity =  -0.03655968\n",
      "Sentence =  The output and codes will be uploaded in GitHub ; similarity =  0.13373005\n",
      "Sentence =  Word embedding to Sentence embedding ; similarity =  0.38792998\n",
      "Sentence =  Word Embedding ; similarity =  0.35745233\n",
      "Sentence =  To perform natural language processing and text mining words are represented as vectors which can be represented in multi-dimensional space ; similarity =  0.23304111\n",
      "Sentence =  This has already been implemented in some pre trained model like WordVec, Glove etc.[] ; similarity =  -0.06950799\n",
      "Sentence =  Drawbacks of word embedding ; similarity =  0.4250577\n",
      "Sentence =  Lot of times it happens that only tagging the word to a vector and inferring is not enough as words have their own contextual meaning and the position of a word in a language plays an important role ; similarity =  0.22703986\n",
      "Sentence =  So to make the system understand the semantic of a sentence ; similarity =  0.3438061\n",
      "Sentence =  So many models live WordVec has been extended to models like DocVec.[] ; similarity =  -0.09838462\n",
      "Sentence =  Sentence Embedding ; similarity =  0.5001782\n",
      "Sentence =  The new way of representing text become sentence encoding in which a long sentence is not directly hot encoded with one single vector, rather the weighted average of the of individual words is being considered while embedding the sentence with dimensionality reduction.[] ; similarity =  0.4891677\n",
      "Sentence =  Sentence Embedding Techniques ; similarity =  0.43734777\n",
      "Sentence =  DocVec[] ; similarity =  0.08238687\n",
      "Sentence =  The working principle of DocVec is based on the representation of paragraph vector ; similarity =  0.03154272\n",
      "Sentence =  This kind of unsupervised model learns from inferring from subparts of a complete text ; similarity =  0.0524196\n",
      "Sentence =  The length of the paragraph can vary between sentence, paragraph, or complete document ; similarity =  0.17703746\n",
      "Sentence =  This sentence embedding technique can be applied in two ways ; similarity =  0.23576903\n",
      "Sentence =  . ; similarity =  0.02303115\n",
      "Sentence =  Distributed memory model ; similarity =  0.07935934\n",
      "Sentence =  In this type of implementation, the general idea is to predict the most relevant sentence considering the average of word vectors ; similarity =  0.17685407\n",
      "Sentence =  So, while predicting next word or missing word in a sentence it remembers the information collected during the whole process and helps in further inference finding between two different sentences ; similarity =  0.25164926\n",
      "Sentence =  . ; similarity =  0.02303115\n",
      "Sentence =  Distributed bag of words ; similarity =  0.1987589\n",
      "Sentence =  The learning technique of this model is completely opposite to the above mention one ; similarity =  -0.12345658\n",
      "Sentence =  It takes up a random sample word and tries to predict its nearby windows with the help of similar occurring words in the document ; similarity =  0.07802765\n",
      "Sentence =  Figure : Steps in PVDOBW ; similarity =  0.018356176\n",
      "Sentence =  Source: https://cs.stanford.edu/~quocle/paragraph_vector.pdf ; similarity =  0.13114467\n",
      "Sentence =  Sentence-BERT[] ; similarity =  -0.011424832\n",
      "Sentence =  An extended version of the BERT model is SBERT ; similarity =  -0.06733686\n",
      "Sentence =  In BERT models an input sentence needed to compare with huge number of sentences that are already present which is been avoided and the time complexity   is reduced to milliseconds from hours in Sentence-BERT ; similarity =  0.21357632\n",
      "Sentence =  This technique can be used for both regression as well as classification tasks ; similarity =  -0.060458455\n",
      "Sentence =  While modeling for regression tasks cosine similarity of the embedding vector is considered and SoftMax classification technique for classification tasks ; similarity =  -0.048414204\n",
      "Sentence =  Universal Sentence Encoder ; similarity =  0.49029958\n",
      "Sentence =  With developments around sentence embedding Google came up with this library mainly for the purpose of transfer learning to speed up training time by reducing the required supervised training data ; similarity =  0.2451104\n",
      "Sentence =  There are two type of model architecture based one transformer-based sentence encoding and Deep Averaging Network based encoding.[] ; similarity =  0.38542235\n",
      "Sentence =  . ; similarity =  0.02303115\n",
      "Sentence =  Transformer-based sentence encoding model ; similarity =  0.5822705\n",
      "Sentence =  For both of model sentences are being cleaned and tokenized before feeding them to the model ; similarity =  0.071068764\n",
      "Sentence =  In case of transformer-based model, which is built on recurrent or convolution neural network ; similarity =  0.04058722\n",
      "Sentence =  So, for every layer the model keeps on adding different information also known attention addition for layers ; similarity =  -0.020714428\n",
      "Sentence =  [] ; similarity =  0.02303115\n",
      "Sentence =  . ; similarity =  0.02303115\n",
      "Sentence =  Deep Averaging Network ; similarity =  0.11397945\n",
      "Sentence =  In case of deep average network, the embeddings consider the vector average associated with the input sentences ; similarity =  0.13936697\n",
      "Sentence =  Eventually after passing that multi-dimensional vector representation through the sequential layers of the model, the resulting classification is done in the last layer of the neural network.[] ; similarity =  0.034844086\n",
      "Sentence =  Model implementation and Output ; similarity =  0.020553444\n",
      "Sentence =  Sample sentences list as training set ; similarity =  0.22223161\n",
      "Sentence =  Sample sentences as test ; similarity =  0.33428362\n",
      "Sentence =  “I had pizza and pasta” ; similarity =  -0.006316291\n",
      "Sentence =  Model DocVec ; similarity =  -0.061742283\n",
      "Sentence =  Model Sentence-BERT ; similarity =  -0.056466836\n",
      "Sentence =  Model Universal Sentence Encoder ; similarity =  0.31280023\n",
      "Sentence =  Future Scope ; similarity =  0.0010841526\n"
     ]
    }
   ],
   "source": [
    "sentnece = []\n",
    "score= []\n",
    "for sent in txtfile2:\n",
    "  sim = cosine(query_vec, model([sent])[0])\n",
    "  print(\"Sentence = \", sent, \"; similarity = \", sim)\n",
    "  sentnece.append(sent)\n",
    "  score.append(sim)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Transformer-based sentence encoding model</td>\n",
       "      <td>0.582271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence Embedding</td>\n",
       "      <td>0.500178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Universal Sentence Encoder</td>\n",
       "      <td>0.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The new way of representing text become senten...</td>\n",
       "      <td>0.489168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence Embedding Techniques</td>\n",
       "      <td>0.437348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Model DocVec</td>\n",
       "      <td>-0.061742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>An extended version of the BERT model is SBERT</td>\n",
       "      <td>-0.067337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This has already been implemented in some pre ...</td>\n",
       "      <td>-0.069508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>So many models live WordVec has been extended ...</td>\n",
       "      <td>-0.098385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The learning technique of this model is comple...</td>\n",
       "      <td>-0.123457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence     Score\n",
       "50          Transformer-based sentence encoding model  0.582271\n",
       "23                                 Sentence Embedding  0.500178\n",
       "46                         Universal Sentence Encoder  0.490300\n",
       "24  The new way of representing text become senten...  0.489168\n",
       "25                      Sentence Embedding Techniques  0.437348\n",
       "..                                                ...       ...\n",
       "63                                       Model DocVec -0.061742\n",
       "42     An extended version of the BERT model is SBERT -0.067337\n",
       "18  This has already been implemented in some pre ... -0.069508\n",
       "22  So many models live WordVec has been extended ... -0.098385\n",
       "37  The learning technique of this model is comple... -0.123457\n",
       "\n",
       "[67 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_score = pd.DataFrame()\n",
    "df_score[\"Sentence\"] = sentnece\n",
    "df_score[\"Score\"] = score\n",
    "df_score.sort_values(\"Score\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
